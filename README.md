# Awesome-Deepfake-Detection-I-Read
Just I read, share rose to get fun.
# 目录
- [LAA-Net: Localized Artifact Attention Network for Quality-Agnostic and Generalizable Deepfake Detection, CVPR 2024](#LAA)
- [Unlocking the Hidden Potential of CLIP in Generalizable Deepfake Detection, arXiv 2025](#2025CLIP)
<span id="LAA"></span>
## LAA-Net: Localized Artifact Attention Network for Quality-Agnostic and Generalizable Deepfake Detection, CVPR 2024: [Paper](https://arxiv.org/pdf/2401.13856) [Code](https://github.com/10Ring/LAA-Net)
---

### **一、深度伪造检测的挑战**
深度伪造技术（如换脸视频）越来越逼真，传统检测方法面临两大难题：
1. **高质量伪造的隐蔽性**：最新技术生成的伪造视频在**整体画面**（如肤色、光照）上几乎无破绽，但可能在**局部区域**（如发际线边缘、瞳孔反光）留下细微的“数字指纹”（称为**伪影**）。
2. **泛化能力不足**：现有模型在训练时见过的伪造类型上表现良好，但遇到**新型伪造技术**（如未知的生成算法）时，检测准确率大幅下降。

---

### **二、LAA-Net的核心创新**
论文提出LAA-Net（局部伪影注意力网络），通过两种关键技术解决上述问题：

#### **1. 显式注意力机制：让模型学会“聚焦”关键区域**
传统方法依赖模型自动学习关注区域（隐式注意力），但这种方式可能忽略关键细节。LAA-Net通过**多任务学习框架**，强制模型关注伪影易发区域：
- **任务1：真假分类**  
  基本任务，判断输入图像是真实还是伪造。
- **任务2：热图回归（Heatmap Regression）**  
  生成一张“热图”，高亮显示图像中可能包含伪影的区域（例如人脸边缘）。  
  **如何实现？**  
  - 通过模拟伪造过程生成训练数据：将两张真实图像混合，人为制造伪影区域（如图1）。  
  - 热图的生成基于混合区域的边界，利用高斯分布模拟伪影的扩散范围（如图2）。
- **任务3：自一致性回归（Self-Consistency Regression）**  
  检查图像不同区域的一致性。例如，若左脸光照与右脸不协调，可能暴露伪造痕迹。  
  **如何实现？**  
  - 随机选择一个伪影点，计算该点与周围区域的相似性，生成一致性矩阵作为监督信号。

**意义**：通过多任务学习，模型不仅学习判断真假，还“被迫”理解伪造的局部特征，提升对未知伪造技术的适应能力。

---

#### **2. 增强特征金字塔网络（E-FPN）：保留细节，减少冗余**
传统神经网络（如ResNet、EfficientNet）通过层层卷积提取特征，但低层细节（如纹理、边缘）会逐渐丢失。LAA-Net提出**E-FPN**，改进特征金字塔结构：
- **传统FPN的问题**：直接融合多层特征会导致冗余（重复信息），影响检测效率。
- **E-FPN的改进**：  
  - **特征过滤**：通过Sigmoid函数生成权重，抑制冗余特征，保留关键细节。  
  - **多尺度融合**：将不同层级的特征（高分辨率细节 + 高层语义信息）动态结合，确保伪影相关的低层信息传递到最终分类层。

**类比**：想象用不同倍数的显微镜观察同一张图片，E-FPN的作用是将所有倍数的关键信息综合，避免遗漏任何细节。

---

### **三、实验与结果**
#### **1. 数据集与指标**
- **训练数据**：FF++（包含4种常见伪造类型）。  
- **测试数据**：Celeb-DFv2（高质量伪造）、DFDC（复杂场景）等，涵盖不同质量的伪造内容。
- **评价指标**：  
  - **AUC**：衡量模型区分真假的能力（值越接近1越好）。  
  - **AP**：综合精度指标，考虑不同阈值下的表现。

#### **2. 关键结果**
- **跨数据集测试**（检测未知伪造类型）：  
  LAA-Net在Celeb-DFv2（高质量）上AUC达95.4%，远超传统方法（如Xception的61.18%）。  
- **抗干扰测试**：  
  对图像加入噪声、模糊等干扰后，LAA-Net在多数情况下保持稳定（如模糊干扰下AUC 98.22%），但对高斯噪声敏感（AUC降至53.9%）。

#### **3. 可视化验证**
- **热图与注意力区域**：  
  通过Grad-CAM可视化，LAA-Net能精准定位伪造区域（如嘴唇边缘），而传统方法可能误判背景或无关区域（如图5）。
- **E-FPN vs 传统FPN**：  
  E-FPN的特征融合更聚焦伪影区域，减少冗余激活（如图8）。

---

### **四、总结与展望**
#### **1. 为什么LAA-Net有效？**
- **显式注意力**：通过多任务强制模型学习局部伪影模式，而非依赖隐式特征。
- **特征保留**：E-FPN确保低层细节（如纹理异常）不被丢失，提升检测敏感度。

#### **2. 未来方向**
- **抗噪声优化**：例如结合去噪算法，提升对高斯噪声的鲁棒性。
- **时序信息利用**：当前仅处理单帧图像，未来可结合视频前后帧的动态特征。
---

**类比总结**：  
LAA-Net像一个经验丰富的鉴宝师，不仅观察文物的整体（全局特征），还会用放大镜仔细检查接缝、刻痕等细节（局部伪影），甚至比对不同部位的材质一致性（自一致性）。这种“细节+全局”的双重验证，让它能识破更高明的赝品。
在 Markdown 中，可以通过以下方式插入超链接和生成目录：
<span id="2025CLIP"></span>
## Unlocking the Hidden Potential of CLIP in Generalizable Deepfake Detection, arXiv 2025: [Paper](https://arxiv.org/pdf/2503.19683), [Code](https://github.com/yermandy/deepfake-detection)
这篇文章详细探讨了如何利用 CLIP 模型（Contrastive Language-Image Pretraining）在深度伪造检测中的潜力，并提出了一种基于 CLIP 的检测方法。文章的核心目标是开发一种通用的深度伪造检测方法，能够在多种数据集和未知的伪造技术上表现良好，同时对 CLIP 模型进行最小的修改。

### **1. 提出的方法**
#### **1.1 基线模型**
文章使用 CLIP 的 ViT-L/14 视觉编码器作为基线模型。CLIP 模型以其在新数据集和生成模型上的泛化能力而闻名，并且对后处理具有很强的鲁棒性。基线模型通过冻结 CLIP 的所有权重，仅训练一个简单的线性二分类器来实现。

#### **1.2 数据预处理**
为了优化面部图像的处理，文章采用了 DeepfakeBench 提出的预处理流程，包括：
1. 从每个视频中均匀采样 32 帧。
2. 使用 DLIB 检测器提取最大的人脸。
3. 对齐人脸并计算边界框。
4. 扩展边界框并裁剪为 256×256 的 RGB 图像。
5. 最终输入到 CLIP 模型的大小为 224×224。

#### **1.3 参数高效的微调（PEFT）**
文章采用了参数高效的微调（PEFT）技术，特别是 LN-tuning（Layer Normalization Tuning），仅调整模型的一小部分参数，从而保留 CLIP 的预训练知识，减少计算成本，并降低过拟合的风险。

#### **1.4 特征正则化**
为了提高模型的泛化能力，文章对特征进行 L2 归一化，将特征投影到超球面上。这种正则化方法有助于模型在未见数据集和伪造方法上表现更好。

#### **1.5 超球面上的度量学习**
文章通过引入额外的约束（如均匀性和一致性损失）来进一步优化特征分布，使同类特征更紧密，不同类特征更分散。

#### **1.6 潜在空间增强**
文章通过球面线性插值（slerp）对特征进行增强，以丰富特征空间，防止过拟合，并提高模型对未知生成技术的鲁棒性。

### **2. 所做的实验**
#### **2.1 数据集**
文章使用了 FaceForensics++（FF++）数据集进行训练，并在多个测试数据集上进行评估，包括 Celeb-DF-v2、DFDC、Google’s DFD、FFIW 和 DeepSpeak v1.0。

#### **2.2 评估指标**
文章使用视频级别的 AUROC（Area Under the ROC Curve）作为主要评估指标。为了计算视频级别的分数，文章对每个视频中采样的 32 帧的预测结果取平均值。

#### **2.3 训练设置**
文章使用 CLIP-ViT-L/14 的视觉部分作为主骨干网络，采用 Adam 优化器，学习率从 8e-5 开始，并使用余弦衰减。批量大小设置为 128。

#### **2.4 图像增强**
文章在训练集中应用了随机水平翻转、随机仿射变换、高斯模糊、颜色抖动和 JPEG 压缩等增强方法。

#### **2.5 实验结果**
文章的实验结果表明，所提出的方法在多个基准测试中表现优异，与更复杂的状态方法相当或更好。例如，在 Celeb-DF-v2 上的 AUROC 为 96.62，在 DFD 上为 98.0，在 DFDC 上为 87.15，在 FFIW 上为 91.52，在 DeepSpeak v1.0 上为 92.01。

#### **2.6 消融实验**
文章通过消融实验验证了每个组件的贡献：
1. **Linear Probing**：仅添加一个线性分类器。
2. **LN-Tuning**：除了训练线性分类器外，还解冻层归一化层。
3. **LN-Tuning + Norm**：添加 L2 归一化。
4. **LN-Tuning + Norm + UnAl**：添加均匀性和一致性损失。
5. **LN-Tuning + Norm + UnAl + Slerp**：添加 slerp 增强。

实验结果表明，每个组件都对模型的正则化和泛化能力有积极影响。

### **3. 结论**
文章展示了 CLIP 模型在深度伪造检测中的潜力，并提出了一种简单而强大的基线方法。通过参数高效的微调和正则化技术，该方法在多个基准测试中表现出色。尽管如此，文章也指出了一些挑战，例如在 DFDC 数据集上的性能可以进一步提高，并建议未来的工作可以探索多模态方法和时空信息。
